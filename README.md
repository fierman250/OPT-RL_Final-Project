# SpaceShip Reinforcement Learning Project

A comprehensive reinforcement learning implementation using Double Deep Q-Network (DDQN) with Prioritized Experience Replay to train an AI agent to play a SpaceShip game. This project demonstrates advanced RL techniques including frame stacking, reward shaping, and neural network optimization.

## ï¿½ï¿½ Project Overview

This project implements a sophisticated reinforcement learning system that trains an AI agent to master a SpaceShip arcade game. The agent learns to navigate through asteroid fields, shoot enemies, collect power-ups, and maximize its score through trial and error.

### Key Features

- **Double Deep Q-Network (DDQN)** with Prioritized Experience Replay
- **Frame Stacking** for temporal information processing
- **Advanced Reward Shaping** with multiple reward components
- **Convolutional Neural Network** architecture for visual processing
- **Checkpoint System** for training resumption
- **Comprehensive Logging** and visualization tools
- **Real-time Gameplay Recording** with video output

## ï¿½ï¿½ Game Environment

The SpaceShip game environment features:

- **Player Spaceship**: Controllable ship with health and weapon systems
- **Asteroids**: Randomly generated obstacles with varying sizes and speeds
- **Power-ups**: Shield and weapon upgrades that spawn from destroyed asteroids
- **Scoring System**: Points awarded for destroying asteroids
- **Health System**: Player health decreases on collision with asteroids

### Game Mechanics

- **Actions**: Stay, Move Left, Move Right, Shoot
- **Objectives**: Survive as long as possible while maximizing score
- **Challenges**: Avoid asteroids, shoot enemies, collect power-ups
- **Termination**: Game ends when health reaches zero or score reaches 10,000

## ğŸ§  AI Architecture

### Neural Network Design

The DDQN architecture consists of:

1. **Feature Extraction Layers**:
   - Conv2D (4â†’32, 8Ã—8, stride 4)
   - Conv2D (32â†’64, 4Ã—4, stride 2)
   - Conv2D (64â†’64, 3Ã—3, stride 1)
   - Flatten layer

2. **Q-Value Estimation Layers**:
   - Linear (3136â†’512)
   - Linear (512â†’256)
   - Linear (256â†’4 actions)

### Training Features

- **Frame Stacking**: 4 consecutive frames for temporal awareness
- **Image Preprocessing**: Grayscale conversion and resizing to 84Ã—84
- **Prioritized Experience Replay**: Efficient sampling of important experiences
- **Target Network**: Stable learning with periodic updates
- **Epsilon-Greedy Exploration**: Balanced exploration vs exploitation

## ï¿½ï¿½ Reward System

The reward function incorporates multiple components:

```python
reward = 0.1  # Base survival reward

# Score-based reward
if score_delta > 0:
    reward += score_delta * 0.5

# Collision penalty
if collision:
    reward -= 25.0

# Near miss reward
if rock_passed_close:
    reward += 2.0

# Power-up reward
if power_up_collected:
    reward += 20.0
```

## ğŸ› ï¸ Installation & Setup

### Prerequisites

```bash
pip install torch torchvision
pip install pygame
pip install numpy matplotlib pandas
pip install imageio scipy
```

### Project Structure

```bash
OPT-RL/
â”œâ”€â”€ Final_Project/
â”‚ â”œâ”€â”€ RL_Final_Project_v18G.py # Main training script
â”‚ â”œâ”€â”€ Game_EnvB.py # Game environment
â”‚ â”œâ”€â”€ space_ship_game_RL_V2/ # Game assets
â”‚ â”‚ â”œâ”€â”€ img/ # Game images
â”‚ â”‚ â”œâ”€â”€ sound/ # Game sounds
â”‚ â”‚ â””â”€â”€ font.ttf # Game font
â”‚ â””â”€â”€ runs_v18G/ # Training outputs
â”‚ â”œâ”€â”€ best_ddqn_space_ship_v18G.pth
â”‚ â”œâ”€â”€ checkpoint.pth
â”‚ â”œâ”€â”€ training_data.xlsx
â”‚ â””â”€â”€ .mp4 # Gameplay videos


## ğŸš€ Usage

### Training Mode

```python
# Run the main script
python RL_Final_Project_v18G.py

# Select mode: TRAIN
```

### Evaluation Mode

```python
# Run the main script
python RL_Final_Project_v18G.py

# Select mode: EVAL
```

### Training + Evaluation

```python
# Run the main script
python RL_Final_Project_v18G.py

# Select mode: TRAIN_EVAL
```

## âš™ï¸ Hyperparameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| Episodes | 4000 | Total training episodes |
| Batch Size | 256 | Training batch size |
| Learning Rate | 1e-4 | Adam optimizer learning rate |
| Gamma | 0.99 | Discount factor |
| Epsilon Start | 1.0 | Initial exploration rate |
| Epsilon End | 0.05 | Final exploration rate |
| Memory Size | 50000 | Replay buffer capacity |
| Target Update | 1000 | Target network update frequency |

## ï¿½ï¿½ Training Results

The training process includes:

- **Real-time Logging**: Episode rewards, scores, and metrics
- **Checkpoint System**: Automatic model saving and resumption
- **Visualization**: Training curves and performance plots
- **Video Recording**: Gameplay demonstrations

### Performance Metrics

- **Total Reward**: Cumulative reward per episode
- **Average Reward**: Mean reward per step
- **Game Score**: In-game score achieved
- **Training Loss**: Neural network loss during training
- **Q-Value Statistics**: Max and mean Q-values

## ï¿½ï¿½ Key Innovations

1. **Advanced Reward Shaping**: Multi-component reward system for better learning
2. **Frame Stacking**: Temporal information processing for better decision making
3. **Prioritized Experience Replay**: Efficient learning from important experiences
4. **Checkpoint System**: Robust training with automatic resumption
5. **Comprehensive Logging**: Detailed tracking of training progress

## ğŸ“ Output Files

After training, the system generates:

- **Model Files**: Best performing model weights
- **Checkpoints**: Training state for resumption
- **Training Data**: Excel file with all metrics
- **Visualizations**: Training curves and performance plots
- **Gameplay Videos**: MP4 recordings of agent performance

## ï¿½ï¿½ Customization

### Modifying Reward Function

Edit the `_calculate_reward` method in `SpaceShipEnv` class:

```python
def _calculate_reward(self, action, score):
    reward = 0.1  # Base reward
    # Add your custom reward components
    return reward
```

### Adjusting Hyperparameters

Modify the hyperparameters section in the main script:

```python
EPISODES = 4000
BATCH_SIZE = 256
GAMMA = 0.99
# ... other parameters
```

### Changing Network Architecture

Modify the `DDQN` class to experiment with different architectures:

```python
class DDQN(nn.Module):
    def __init__(self, action_dim, input_channels=4):
        # Customize your network layers
        pass
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues for:

- Bug fixes
- Performance improvements
- New features
- Documentation updates

## ğŸ“„ License

This project is part of the Optimization and Reinforcement Learning course. Please refer to your course guidelines for usage and distribution.

## ï¿½ï¿½ Acknowledgments

- Course instructors for guidance on RL concepts
- PyTorch community for excellent documentation
- Pygame developers for the game framework
- OpenAI Gym for inspiration on environment design

---

**Note**: This project demonstrates advanced reinforcement learning techniques and serves as a comprehensive example of implementing DDQN with modern best practices for game AI development.

